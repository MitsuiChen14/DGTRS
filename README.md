# LRSCLIP

This is the open-source repository for the paper ã€Š**LRSCLIP: A Vision-Language Foundation Model for Aligning Remote Sensing Images with Longer Text**ã€‹.

Weizhi Chen, Jingbo Chen, Yupeng Deng*, Jiansheng Chen*, Yuman Feng, Zhihao Xi, Diyou Liu, Kai Li, Yu Meng

## Highlights

* ðŸ”¥LRSCLIP is the first vision-language foundation model in the remote sensing domain that can simultaneously handle both long and short texts.
* ðŸ”¥LRS2M is the first image-text dataset in the remote sensingdomain with 2 million data points that simultaneously provides both long and short text captions for images.
* ðŸ”¥LRSCLIP achieves SOTA performance in four zero-shot downstream tasks: long-text cross-modal retrieval, short-text cross-modal retrieval, image classification, and semantic localization.

## News



## Usage

### Installation

Our model is based on [Long-CLIP](https://github.com/beichenzbc/Long-CLIP), please prepare environment for CLIP.

### how to sue

Please first clone our [repo](https://github.com/MitsuiChen14/LRSCLIP) from github by running the following command.

```shell
git clone https://github.com/MitsuiChen14/LRSCLIP.git
cd LRSCLIP
```

Then, download the LRSCLIP-B/16 model weights and place them in the `./checkpoints` directory.



## Evaluation



## Citation

If you find our work helpful for your research, please consider giving a citation:

```

```









